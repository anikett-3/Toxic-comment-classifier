# Toxic Comment Classification using ML & NLP

This project is a Machine Learning and Natural Language Processing (NLP) based application that classifies text comments as **toxic** or **non-toxic**. The system is designed to help identify harmful language in user-generated content.

---

## ğŸ” Problem Statement
Online platforms receive a large number of user comments daily. Some of these comments may contain toxic or abusive language. Manual moderation is inefficient, so this project aims to automate toxic comment detection using machine learning.

---

## ğŸ¯ Project Objectives
- To preprocess textual data using NLP techniques  
- To extract meaningful features using TF-IDF  
- To train a machine learning model for toxic comment classification  
- To evaluate the model using standard performance metrics  
- To deploy the model as a web application  

---

## ğŸ§  Approach
1. Text preprocessing (lowercasing, stopword removal, cleaning)
2. Feature extraction using TF-IDF
3. Model training using Logistic Regression
4. Model evaluation using accuracy, precision, recall, and F1-score
5. Deployment using Streamlit

---

## ğŸ› ï¸ Tech Stack
- Python  
- Pandas, NumPy  
- NLTK  
- Scikit-learn  
- TF-IDF  
- Logistic Regression  
- Streamlit  
- Google Colab  
- Hugging Face Spaces  

---

## ğŸ“Š Dataset
The dataset used is the **Jigsaw Toxic Comment Classification Dataset**, which contains Wikipedia comments labeled for toxicity.  
For simplicity, binary classification (`toxic` / `non-toxic`) is used in this project.

---

## ğŸš€ Live Demo
ğŸ‘‰ https://huggingface.co/spaces/ani-ket03/Toxic-comment-classifier

---

## ğŸ–¼ï¸ Screenshots
### Application Home
![App Home](Home.png)  or

<img width="1910" height="871" alt="Home" src="https://github.com/user-attachments/assets/cec8d156-78ab-4217-8a3e-e98325631743" />

### Toxic Comment Prediction
![Toxic Result](Toxic.png)  or

<img width="1896" height="834" alt="Toxic" src="https://github.com/user-attachments/assets/9c665485-19e4-439b-beb1-78367cb9890c" />


### Non-Toxic Comment Prediction
![Non-Toxic Result](Non_toxic.png) or 

<img width="1909" height="875" alt="Non_toxic" src="https://github.com/user-attachments/assets/53156ce3-f821-4f48-aa25-a992f0d15d4c" />



---

ğŸ‘¤ Author

Aniket Kumar
B.Tech Student

ğŸ“œ License

This project is licensed under the MIT License.








